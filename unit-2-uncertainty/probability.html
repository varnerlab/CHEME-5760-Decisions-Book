
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Rational Uncertain Decisions &#8212; CHEME 5760</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="canonical" href="https://varnerlab.github.io/CHEME-5760-Decisions-Book/landing.html/unit-2-uncertainty/probability.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Risk and Risk Aversion" href="risk.html" />
    <link rel="prev" title="Decision-making in uncertain situations" href="uncertainty-landing.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-VQRVBL1C02"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-VQRVBL1C02');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/cornell_seal_simple_black.svg" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">CHEME 5760</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../landing.html">
                    Quantitative Decisions in Life, Love, and Finance
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../unit-1-simpledecisions/simpledecisions-landing.html">
   Unit 1. Utility and Simple Optimal Decisions
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-1-simpledecisions/utilityfunctions.html">
     Utility Functions and Rational Choices
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-1-simpledecisions/utlitymaximization.html">
     Maximizing Utility Subject to Constraints
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="uncertainty-landing.html">
   Unit 2. Rational Uncertain Optimal Decisions
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Rational Uncertain Decisions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="risk.html">
     Risk and Risk Aversion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="games.html">
     Multiagent Reasoning and Game Theory
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../unit-3-planning/planning-landing.html">
   Unit 3. Sequential Planning and Decisions
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-3-planning/markov.html">
     Markov Chains
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-3-planning/mdp.html">
     Markov Decision Processes
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../unit-4-learning/learning-landing.html">
   Unit 4. Machine Learning Methods and Decisions
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-4-learning/rl.html">
     Reinforcement Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../unit-4-learning/imitation.html">
     Imitation Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../References.html">
   References
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/varnerlab/CHEME-5760-Decisions-Book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/varnerlab/CHEME-5760-Decisions-Book/issues/new?title=Issue%20on%20page%20%2Funit-2-uncertainty/probability.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/unit-2-uncertainty/probability.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Rational Uncertain Decisions
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-von-neumann-morgenstern-theorem">
     The von Neumann-Morgenstern theorem
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#expected-utility-problem">
       Expected utility problem
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-variables-and-probability">
     Random variables and probability
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#probability-spaces">
       Probability spaces
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sample-space-omega">
         Sample space
         <span class="math notranslate nohighlight">
          \(\Omega\)
         </span>
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#event-space-mathcal-f">
         Event space
         <span class="math notranslate nohighlight">
          \(\mathcal{F}\)
         </span>
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#probability-law-p">
         Probability law
         <span class="math notranslate nohighlight">
          \(P\)
         </span>
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#conditional-probability">
         Conditional Probability
        </a>
        <ul class="nav section-nav flex-column">
         <li class="toc-h5 nav-item toc-entry">
          <a class="reference internal nav-link" href="#independence-versus-disjoint">
           Independence versus Disjoint
          </a>
         </li>
        </ul>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bayes-theorem-and-the-law-of-total-probability">
       Bayes’ theorem and the law of total probability
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#law-of-total-probability">
       Law of Total Probability
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#probability-mass-functions">
       Probability mass functions
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#bernoulli-random-variable">
         Bernoulli random variable
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#binomial-random-variable">
         Binomial random variable
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#geometric-random-variable">
         Geometric random variable
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#poisson-random-variable">
         Poisson random variable
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#moments-of-a-random-variable">
     Moments of a random variable
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#expectation">
       Expectation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#variance">
       Variance
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Rational Uncertain Decisions</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Rational Uncertain Decisions
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-von-neumann-morgenstern-theorem">
     The von Neumann-Morgenstern theorem
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#expected-utility-problem">
       Expected utility problem
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#random-variables-and-probability">
     Random variables and probability
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#probability-spaces">
       Probability spaces
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#sample-space-omega">
         Sample space
         <span class="math notranslate nohighlight">
          \(\Omega\)
         </span>
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#event-space-mathcal-f">
         Event space
         <span class="math notranslate nohighlight">
          \(\mathcal{F}\)
         </span>
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#probability-law-p">
         Probability law
         <span class="math notranslate nohighlight">
          \(P\)
         </span>
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#conditional-probability">
         Conditional Probability
        </a>
        <ul class="nav section-nav flex-column">
         <li class="toc-h5 nav-item toc-entry">
          <a class="reference internal nav-link" href="#independence-versus-disjoint">
           Independence versus Disjoint
          </a>
         </li>
        </ul>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bayes-theorem-and-the-law-of-total-probability">
       Bayes’ theorem and the law of total probability
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#law-of-total-probability">
       Law of Total Probability
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#probability-mass-functions">
       Probability mass functions
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#bernoulli-random-variable">
         Bernoulli random variable
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#binomial-random-variable">
         Binomial random variable
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#geometric-random-variable">
         Geometric random variable
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#poisson-random-variable">
         Poisson random variable
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#moments-of-a-random-variable">
     Moments of a random variable
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#expectation">
       Expectation
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#variance">
       Variance
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#summary">
   Summary
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="rational-uncertain-decisions">
<h1>Rational Uncertain Decisions<a class="headerlink" href="#rational-uncertain-decisions" title="Permalink to this headline">#</a></h1>
<p>Previously, we developed tools to make optimal choices when the outcomes were sure, and the level of satisfaction derived from those choices was known, e.g., the utility of purchasing a particular bundle of goods or services could be computed using a utility function. However, in many real-world situations, the assumption of certainty is invalid. For example, betting, buying an insurance policy, investing in a new business, or the stock market all or <em>uncertain</em>.</p>
<p>Toward understanding how to make uncertain rational decisions, we begin by introducing the <a class="reference external" href="https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem">von Neumann-Morgenstern theorem</a> and then discuss tools to model uncertainty:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#content-references-vnm-theorem"><span class="std std-ref">The von Neumann-Morgenstern theorem</span></a> is a fundamental concept in decision theory that provides a mathematical foundation for expected utility theory. It states that if an individual’s preferences over uncertain outcomes satisfy certain axioms, then there exists a utility function that represents those preferences. This theorem has important implications for understanding how people make decisions under uncertainty and designing mechanisms to elicit and aggregate individual preferences.</p></li>
<li><p><a class="reference internal" href="#content-references-random-variables-probability"><span class="std std-ref">Random variables and probability</span></a> are central concepts in understanding uncertainty.  Random variables represent quantities that can take on different values with a particular probability distribution. They are used to model uncertainty and randomness in various fields, including finance, physics, and engineering. Probability theory provides a framework for understanding and quantifying the likelihood of different events occurring based on the underlying distribution of the random variables involved.</p></li>
</ul>
<hr class="docutils" />
<section id="the-von-neumann-morgenstern-theorem">
<span id="content-references-vnm-theorem"></span><h2>The von Neumann-Morgenstern theorem<a class="headerlink" href="#the-von-neumann-morgenstern-theorem" title="Permalink to this headline">#</a></h2>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem">von Neumann-Morgenstern theorem</a>, the basis for the <a class="reference external" href="https://en.wikipedia.org/wiki/Expected_utility_hypothesis">expected utility hypothesis</a>, is a fundamental result in decision theory that provides a framework for making <em>rational choices</em> under uncertainty <span id="id1">[<a class="reference internal" href="../References.html#id2" title="J. von Neumann and O. Morgenstern. Theory of games and economic behavior. Princeton University Press, 1944.">von Neumann and Morgenstern, 1944</a>]</span>.</p>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem">von Neumann-Morgenstern theorem</a> posits, under certain axioms of rational behavior, that an agent faced with probabilistic outcomes will behave as if they are maximizing the expected value of a von Neumann–Morgenstern utility function, defined over the potential outcomes, allowing decision-makers to make choices that maximize their expected utility.</p>
<p>The rationality of the <a class="reference external" href="https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem">von Neumann-Morgenstern theorem</a> is based on three axioms:</p>
<ul class="simple">
<li><p><strong>Completeness</strong>: The decision maker can always compare any two alternatives and determine which one is preferred or declare them equally preferred.</p></li>
<li><p><strong>Transitivity</strong>: If alternative A is preferred to alternative B, and alternative B is preferred to alternative C, then alternative A must be preferred to alternative C.</p></li>
<li><p><strong>Independence</strong>: The utility of an alternative should depend only on the consequences and not on how the decision maker arrived at that alternative. Specifically, if alternatives A and B have the same values, the decision maker should be indifferent between them, regardless of how the choice was presented or framed.</p></li>
</ul>
<!-- 
The VNM utility theorem shows that any individual who follows these axioms must have a utility function that can be used to evaluate and compare alternatives and that a unique probability distribution over outcomes can represent this utility function. -->
<section id="expected-utility-problem">
<h3>Expected utility problem<a class="headerlink" href="#expected-utility-problem" title="Permalink to this headline">#</a></h3>
<p>The world exists in states <span class="math notranslate nohighlight">\(\mathcal{S}\)</span>, agents make observations <span class="math notranslate nohighlight">\(\mathcal{O}\)</span> and take actions <span class="math notranslate nohighlight">\(\mathcal{A}\)</span>.  The utility of state <span class="math notranslate nohighlight">\(s\in\mathcal{S}\)</span> is described by <span class="math notranslate nohighlight">\(U(s)\)</span>, where the utility function <span class="math notranslate nohighlight">\(U\)</span> follows the <a class="reference external" href="https://varnerlab.github.io/CHEME-5760-Decisions-Book/unit-1-simpledecisions/utilityfunctions.html#properties-of-utility-functions">properties of the utility functions</a>. Further, an agent makes observations <span class="math notranslate nohighlight">\(o\in\mathcal{O}\)</span> and takes actions <span class="math notranslate nohighlight">\(a\in\mathcal{A}\)</span>. Then, a rational agent solves the expected utility problem (<a class="reference internal" href="#defn-expected-utility-hypothesis">Definition 4</a>):</p>
<div class="proof definition admonition" id="defn-expected-utility-hypothesis">
<p class="admonition-title"><span class="caption-number">Definition 4 </span> (Expected utility problem)</p>
<section class="definition-content" id="proof-content">
<p>An agent has a model <span class="math notranslate nohighlight">\(P(s^{\prime}|o,a)\)</span> which represents the probability of the world being in state <span class="math notranslate nohighlight">\(s^{\prime}\)</span> given that the agent observes <span class="math notranslate nohighlight">\(o\)</span> and takes action <span class="math notranslate nohighlight">\(a\)</span>. The payoff of state <span class="math notranslate nohighlight">\(s^{\prime}\)</span> is given by <span class="math notranslate nohighlight">\(U(s^{\prime})\)</span>. A <em>rational agent</em> maximizes the expected utility subject to constraints:</p>
<div class="math notranslate nohighlight" id="equation-eqn-max-expected-ulity-problem">
<span class="eqno">(11)<a class="headerlink" href="#equation-eqn-max-expected-ulity-problem" title="Permalink to this equation">#</a></span>\[\begin{split}\begin{eqnarray}
\text{maximize}~\mathbb{E}(s) &amp;=&amp; \sum_{s^{\prime}}P(s^{\prime}|a,o)U(s^{\prime}) \\
\text{subject to}~s^{\prime} &amp;=&amp; T(s,a) \\
\text{and}~s&amp;\in&amp;\mathcal{S} \\
\text{and}~a&amp;\in&amp;\mathcal{A} \\
\text{and}~o&amp;\in&amp;\mathcal{O} \\
\end{eqnarray}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(T(s,a)\)</span> denotes a transition function governing the transition from state <span class="math notranslate nohighlight">\(s\rightarrow{s^{\prime}}\)</span> under action <span class="math notranslate nohighlight">\(a\)</span>.</p>
</section>
</div><p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Expected_utility_hypothesis">expected utility hypothesis</a> depends upon an understanding of two critical concepts, random variables, and probability.</p>
</section>
</section>
<section id="random-variables-and-probability">
<span id="content-references-random-variables-probability"></span><h2>Random variables and probability<a class="headerlink" href="#random-variables-and-probability" title="Permalink to this headline">#</a></h2>
<p>The sample space and the event space are all based on statements, for example, getting a head when flipping a coin, winning the game, or drawing a card, etc. These statements are not numbers; how do we convert a statement to a number? The answer is a random variable; random variables are mappings from events to numbers, these numbers are probabilities.</p>
<ul class="simple">
<li><p>A <a class="reference external" href="https://en.wikipedia.org/wiki/Random_variable">random variable</a> is a variable <span class="math notranslate nohighlight">\(X\)</span> that takes on different values <span class="math notranslate nohighlight">\(x\)</span> according to the outcome of a random event or process. There are two types of random variables: discrete random variables and continuous random variables. Discrete random variables can take on a countable number of distinct values, while continuous random variables can take on any value in a continuous range.</p></li>
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Probability">Probability</a> measures the likelihood that a particular event or outcome will occur and is commonly used to quantify uncertainty in various fields, such as science, engineering, economics, and finance. For a discrete random variable, the likelihood that <span class="math notranslate nohighlight">\(X=x\)</span> is described by a <a class="reference external" href="https://en.wikipedia.org/wiki/Probability_mass_function">Probability Mass Function (PMF)</a> and a <a class="reference external" href="https://en.wikipedia.org/wiki/Probability_density_function">Probability Density Function (PDF)</a> for continuous random variables.</p></li>
</ul>
<section id="probability-spaces">
<span id="content-references-probability"></span><h3>Probability spaces<a class="headerlink" href="#probability-spaces" title="Permalink to this headline">#</a></h3>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Frequentist_probability">Frequentists</a> argue that probability is the relative frequency or propensity of a particular outcome in the set of all possible outcomes. On the other hand, <a class="reference external" href="https://en.wikipedia.org/wiki/Bayesian_probability">Bayesians</a> argue that probability is a subjective belief. The context of your problem will typically suggest which perspective to use. For example, when you have a shortage of data, a Bayesian approach allows you to use prior knowledge or belief. On the other hand, when in a data-rich environment, a <a class="reference external" href="https://en.wikipedia.org/wiki/Frequentist_probability">frequentist</a> approach calculates the probability of an event directly from the data, along with confidence intervals on your estimate.</p>
<p>Whether you prefer the <a class="reference external" href="https://en.wikipedia.org/wiki/Frequentist_probability">frequentist’s</a> or the <a class="reference external" href="https://en.wikipedia.org/wiki/Bayesian_probability">Bayesian view</a>, there is a more fundamental notion of probability thanks to <a class="reference external" href="https://en.wikipedia.org/wiki/Andrey_Kolmogorov">Andrey Kolmogorov</a>, namely, the probability is a measure of the size of a set (<a class="reference internal" href="#defn-prob-space-kolmogorov">Definition 5</a>):</p>
<div class="proof definition admonition" id="defn-prob-space-kolmogorov">
<p class="admonition-title"><span class="caption-number">Definition 5 </span> (Probability space)</p>
<section class="definition-content" id="proof-content">
<p>A probability space is described by the tuple of objects <span class="math notranslate nohighlight">\((\Omega,\mathcal{F},P)\)</span>:</p>
<ul class="simple">
<li><p>The sample space <span class="math notranslate nohighlight">\(\Omega\)</span> holds the set of all possible outcomes from an experiment.</p></li>
<li><p>The event space <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is the collection of all possible events. An event <span class="math notranslate nohighlight">\(E\)</span> is a subset in <span class="math notranslate nohighlight">\(\Omega\)</span> that defines an outcome or a combination of outcomes.</p></li>
<li><p>The probability law <span class="math notranslate nohighlight">\(P\)</span> is a mapping from an event <span class="math notranslate nohighlight">\(E\)</span> to a number <span class="math notranslate nohighlight">\(P(E)\)</span> which, measures the <em>size</em> of the event <span class="math notranslate nohighlight">\(E\)</span>.</p></li>
</ul>
</section>
</div><section id="sample-space-omega">
<h4>Sample space <span class="math notranslate nohighlight">\(\Omega\)</span><a class="headerlink" href="#sample-space-omega" title="Permalink to this headline">#</a></h4>
<p>Given an experiment, the sample space <span class="math notranslate nohighlight">\(\Omega\)</span> is the set containing all possible outcomes of that experiment. These outcomes can be numbers, alphabets, vectors, or functions, as well as, images, videos, EEG signals, audio speeches, etc.</p>
<p>Let’s consider an example of a six sided dice (<a class="reference internal" href="#ex-sample-space-dice">Example 1</a>):</p>
<div class="proof example dropdown admonition" id="ex-sample-space-dice">
<p class="admonition-title"><span class="caption-number">Example 1 </span> (Sample space <span class="math notranslate nohighlight">\(\Omega\)</span> of six sided dice)</p>
<section class="example-content" id="proof-content">
<p>Suppose we were intersted in the outcome of experiment where a six sided dice was rolled on time.
Then the sample space for this experiment <span class="math notranslate nohighlight">\(\Omega\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\Omega=\left\{1,2,3,4,5,6\right\}\]</div>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Cardinality">cardinality</a> of this sample space <span class="math notranslate nohighlight">\(\dim\left(\Omega\right) = 6\)</span>.</p>
</section>
</div></section>
<section id="event-space-mathcal-f">
<h4>Event space <span class="math notranslate nohighlight">\(\mathcal{F}\)</span><a class="headerlink" href="#event-space-mathcal-f" title="Permalink to this headline">#</a></h4>
<p>The sample space <span class="math notranslate nohighlight">\(\Omega\)</span> contains all the possible outcomes of an experiment.
However, we may not be interested in an individual outcome.
Rather we may be interested in combinations of individual outcomes where the elements of these sets share some common trait, e.g., even integers or the collection of face cards, etc. These subsets are called events <span class="math notranslate nohighlight">\(E\subseteq\Omega\)</span>, and the set of all possible events, denoted as <span class="math notranslate nohighlight">\(\mathcal{F}\)</span>, is called the event space.
Thus, the event space <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is a particular set of sets; it’s the set of all possible subsets.</p>
<p>Let’s enumerate the event space for the four suits of a typical deck of cards (<a class="reference internal" href="#ex-event-card-suits">Example 2</a>):</p>
<div class="proof example dropdown admonition" id="ex-event-card-suits">
<p class="admonition-title"><span class="caption-number">Example 2 </span> (Enumerate the event space <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> )</p>
<section class="example-content" id="proof-content">
<p>Construct the event space <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> for the sample space <span class="math notranslate nohighlight">\(\Omega=\left\{\clubsuit, \diamondsuit, \heartsuit, \spadesuit\right\}\)</span>. The cardinality of <span class="math notranslate nohighlight">\(\Omega\)</span> is <span class="math notranslate nohighlight">\(\dim\left(\Omega\right) = 4\)</span>.</p>
<p><strong>Solution</strong>: The <span class="math notranslate nohighlight">\(\dim(\mathcal{F}) = 2^n\)</span> where the elements of <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> correspond to the first <span class="math notranslate nohighlight">\(\dim(\Omega)\)</span> digits of the binary representation of the integers <span class="math notranslate nohighlight">\(i=0,\dots,\dim(\mathcal{F})-1\)</span>. For example, the bitstring <code class="docutils literal notranslate"><span class="pre">0000</span></code>, which corresponds to <span class="math notranslate nohighlight">\(i=0\)</span>, represents the empty set <span class="math notranslate nohighlight">\(\emptyset\)</span> while the bitstring <code class="docutils literal notranslate"><span class="pre">1111</span></code>, which corresponds to <span class="math notranslate nohighlight">\(i=15\)</span>, corresponds to <span class="math notranslate nohighlight">\(\left\{\clubsuit, \diamondsuit, \heartsuit, \spadesuit\right\}\)</span>.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>index</p></th>
<th class="head"><p>bitstring</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(x\in\mathcal{F}\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0000</span></code></p></td>
<td><p><span class="math notranslate nohighlight">\(\emptyset\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>1</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0001</span></code></p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{\spadesuit\right\}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>2</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0010</span></code></p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{\heartsuit\right\}\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>3</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">0011</span></code></p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{\heartsuit, \spadesuit\right\}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>.</p></td>
<td><p>….</p></td>
<td><p>…</p></td>
</tr>
<tr class="row-odd"><td><p>9</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1001</span></code></p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{\clubsuit, \spadesuit\right\}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>.</p></td>
<td><p>….</p></td>
<td><p>…</p></td>
</tr>
<tr class="row-odd"><td><p>14</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1110</span></code></p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{\clubsuit, \diamondsuit, \heartsuit\right\}\)</span></p></td>
</tr>
<tr class="row-even"><td><p>15</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">1111</span></code></p></td>
<td><p><span class="math notranslate nohighlight">\(\left\{\clubsuit, \diamondsuit, \heartsuit, \spadesuit\right\}\)</span></p></td>
</tr>
</tbody>
</table>
</section>
</div></section>
<section id="probability-law-p">
<h4>Probability law <span class="math notranslate nohighlight">\(P\)</span><a class="headerlink" href="#probability-law-p" title="Permalink to this headline">#</a></h4>
<p>A probability law <span class="math notranslate nohighlight">\(P\)</span> is a function <span class="math notranslate nohighlight">\(P\)</span> : <span class="math notranslate nohighlight">\(\mathcal{F}\rightarrow\left[0, 1\right]\)</span>;
the function <span class="math notranslate nohighlight">\(P\)</span> maps an event (set) <span class="math notranslate nohighlight">\(E\subseteq\Omega\)</span> to a real number in <span class="math notranslate nohighlight">\(\left[0, 1\right]\)</span>.
The definition above does not specify how an event <span class="math notranslate nohighlight">\(E\subseteq\Omega\)</span> is being mapped to a number.
However, since probability is a measure of the size of a set, a meaningful probability law <span class="math notranslate nohighlight">\(P\)</span> should be consistent for all <span class="math notranslate nohighlight">\(E\subseteq\Omega\)</span>.</p>
<p>This requires rules, known as the axioms of probability (<a class="reference internal" href="#axiom-probability">Axiom 1</a>):</p>
<div class="proof axiom admonition" id="axiom-probability">
<p class="admonition-title"><span class="caption-number">Axiom 1 </span> (Axioms of Probability)</p>
<section class="axiom-content" id="proof-content">
<p>A probability law <span class="math notranslate nohighlight">\(P\)</span> is a function <span class="math notranslate nohighlight">\(P:\mathcal{F}\rightarrow\left[0, 1\right]\)</span> that maps an event <span class="math notranslate nohighlight">\(E\subseteq\Omega\)</span>
to a real number on the interval <span class="math notranslate nohighlight">\(\left[0, 1\right]\)</span>. The function <span class="math notranslate nohighlight">\(P\)</span> must satisfy the three axioms of probability:</p>
<ul class="simple">
<li><p>Non-negativity: <span class="math notranslate nohighlight">\(P(E)\geq{0}\)</span>, for any <span class="math notranslate nohighlight">\(E\in\mathcal{F}\)</span></p></li>
<li><p>Normalization: <span class="math notranslate nohighlight">\(P(\Omega)=1\)</span></p></li>
<li><p>Additivity: For any disjoint event sets <span class="math notranslate nohighlight">\(\left\{E_{1}, E_{2}, \dots, E_{n}\right\}\)</span> then <span class="math notranslate nohighlight">\(P\left(\bigcup_{i=1}^{n}E_{i}\right) = 
\sum_{i=1}^{n}P(E_{i})\)</span></p></li>
</ul>
</section>
</div></section>
<section id="conditional-probability">
<h4>Conditional Probability<a class="headerlink" href="#conditional-probability" title="Permalink to this headline">#</a></h4>
<p>The motivation of conditional probability is to restrict the probability to a subevent happening in the sample space. If B has happened, the probability for A to also happen is P[A∩B]/P[B]. If two events are not influencing each other, then we say that A and B are independent.</p>
<section id="independence-versus-disjoint">
<h5>Independence versus Disjoint<a class="headerlink" href="#independence-versus-disjoint" title="Permalink to this headline">#</a></h5>
<p>Conditional probability deals with situations where two events, <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span>, are related.  However, what if the two events are unrelated, i.e., information about one event says nothing about the second event? In this case, the events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are independent (<a class="reference internal" href="#defn-independence">Definition 6</a>):</p>
<div class="proof definition admonition" id="defn-independence">
<p class="admonition-title"><span class="caption-number">Definition 6 </span> (Statistical independence of events)</p>
<section class="definition-content" id="proof-content">
<p>Two events A and B are statistically independent if:</p>
<div class="math notranslate nohighlight">
\[P\left(A\cap{B}\right) = P(A)P(B)\]</div>
</section>
</div><p>However, independence says nothing about whether two events are disjoint. Suppose events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> were disjoint, then we know that <span class="math notranslate nohighlight">\(A\cap{B} = 0\)</span> and:</p>
<div class="math notranslate nohighlight">
\[P\left(A\cap{B}\right) = 0\]</div>
<p>But, this says nothing about whether <span class="math notranslate nohighlight">\(P(A\cap{B})\)</span> can be factorized into the product <span class="math notranslate nohighlight">\(P(A)P(B)\)</span>.
The only case when disjoint implies independence is if either <span class="math notranslate nohighlight">\(P(A) = 0\)</span> or <span class="math notranslate nohighlight">\(P(B) = 0\)</span>.</p>
</section>
</section>
</section>
<section id="bayes-theorem-and-the-law-of-total-probability">
<h3>Bayes’ theorem and the law of total probability<a class="headerlink" href="#bayes-theorem-and-the-law-of-total-probability" title="Permalink to this headline">#</a></h3>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’ theorem</a>, named after <a class="reference external" href="https://en.wikipedia.org/wiki/Thomas_Bayes">Thomas Bayes</a>, describes the likelihood of an event based on prior knowledge of conditions related to the event (<a class="reference internal" href="#bayes-theorem">Theorem 1</a>):</p>
<div class="proof theorem admonition" id="bayes-theorem">
<p class="admonition-title"><span class="caption-number">Theorem 1 </span> (Bayes’ theorem)</p>
<section class="theorem-content" id="proof-content">
<p>For any two events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> where <span class="math notranslate nohighlight">\(P(A) &gt; 0\)</span> and <span class="math notranslate nohighlight">\(P(B) &gt; 0\)</span>, the conditional probability <span class="math notranslate nohighlight">\(P(A\vert{B})\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[P(A\vert{B}) = \frac{P\left(A\cap{B}\right)}{P\left(B\right)}\]</div>
</section>
</div><p>Bayes’ theorem provides two views of the intersection <span class="math notranslate nohighlight">\(P\left(A\cap{B}\right)\)</span> using two different conditional probabilities. To see this, we use the fact that the order of the events <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> is arbitrary:</p>
<div class="math notranslate nohighlight">
\[P(A\,\vert{B})P(B) = P(B\,\vert{A})P(A) = P(A \cap B)\]</div>
<p>Thus, Bayes’ theorem offers a mechanism to interconvert <span class="math notranslate nohighlight">\(P(A\vert{B})\)</span> and <span class="math notranslate nohighlight">\(P(B\vert{A})\)</span>.</p>
</section>
<section id="law-of-total-probability">
<h3>Law of Total Probability<a class="headerlink" href="#law-of-total-probability" title="Permalink to this headline">#</a></h3>
<p>The law of total probability is a fundamental concept in probability theory that allows us to compute the probability of an event by considering all the possible ways in which it can occur (<a class="reference internal" href="#defn-law-total-probability">Theorem 2</a>):</p>
<div class="proof theorem admonition" id="defn-law-total-probability">
<p class="admonition-title"><span class="caption-number">Theorem 2 </span> (Law of Total Probability)</p>
<section class="theorem-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(\left\{A_{1},\dots,A_{n}\right\}\)</span> be a partition of the sample space <span class="math notranslate nohighlight">\(\Omega\)</span> where the
partitions <span class="math notranslate nohighlight">\(A_{\star}\)</span> are disjoint and <span class="math notranslate nohighlight">\(\Omega=A_{1}\cup{A_{2}}\cup\dots\cup{A_{n}}\)</span>.
Then for <span class="math notranslate nohighlight">\(B\subseteq\Omega\)</span>:</p>
<div class="math notranslate nohighlight">
\[P(B) = \sum_{i=1}^{n}P\left(B\,\vert{A_{i}}\right)P\left(A_{i}\right)\]</div>
</section>
</div></section>
<section id="probability-mass-functions">
<h3>Probability mass functions<a class="headerlink" href="#probability-mass-functions" title="Permalink to this headline">#</a></h3>
<p>In the case of discrete random variables, for example, dice roles, coin flips etc, the likelihood that <span class="math notranslate nohighlight">\(X=x\)</span> is described by a <a class="reference external" href="https://en.wikipedia.org/wiki/Probability_mass_function">Probability Mass Function (PMF)</a> (<a class="reference internal" href="#defn-pmf">Definition 7</a>):</p>
<div class="proof definition admonition" id="defn-pmf">
<p class="admonition-title"><span class="caption-number">Definition 7 </span> (Probability Mass Function)</p>
<section class="definition-content" id="proof-content">
<p>The probability mass function (PMF) of a discrete random variable <span class="math notranslate nohighlight">\(X\)</span> is a function that specifies the probability of obtaining <span class="math notranslate nohighlight">\(X = x\)</span>, where <span class="math notranslate nohighlight">\(x\)</span> is a particular event in the set of possible events we’re interested in <span class="math notranslate nohighlight">\(\mathcal{F}\subseteq{X\left(\Omega\right)}\)</span>:</p>
<div class="math notranslate nohighlight">
\[p_{X}(x) = P\left(X=x\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> is the event space, and <span class="math notranslate nohighlight">\(\Omega\)</span> is the sample space. A probability mass function must satisfy the condition:</p>
<div class="math notranslate nohighlight">
\[\sum_{x\in{X(\Omega)}}p_{X}(x)=1\]</div>
</section>
</div><p>Thus, in the context of the <a class="reference external" href="https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem">von Neumann-Morgenstern theorem</a>, the probability mass function is a weight for discrete random variables which model uncertain payoffs.</p>
<p>In <a class="reference external" href="https://julialang.org">Julia</a>, probability mass (or density) functions can be constructed and sampled using the <a class="reference external" href="https://juliastats.org/Distributions.jl/stable/">Distributions.jl</a> package. Let’s look at a few common probability mass functions.</p>
<section id="bernoulli-random-variable">
<h4>Bernoulli random variable<a class="headerlink" href="#bernoulli-random-variable" title="Permalink to this headline">#</a></h4>
<p>A Bernoulli random variable, the simplest random variable, models a coin flip or some other type of binary
outcome (<a class="reference internal" href="#defn-pmf-bernouli">Definition 8</a>):</p>
<div class="proof definition admonition" id="defn-pmf-bernouli">
<p class="admonition-title"><span class="caption-number">Definition 8 </span> (Bernoulli Random Variable)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be a Bernoulli random variable. Then, the probability mass function of <span class="math notranslate nohighlight">\(X\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}p_{X}(x) =
\begin{cases}
  p &amp; \text{if } x = 1 \\
  1 - p &amp; \text{if } x = 0
\end{cases}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(0&lt;p&lt;1\)</span> is called the Bernoulli parameter. For a Bernoulli random variable <span class="math notranslate nohighlight">\(X(\Omega) \in [0,1]\)</span> the expectation is given by:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\left[X\right] = p\]</div>
<p>while the variance <span class="math notranslate nohighlight">\(\text{Var}(X)\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\text{Var}\left[X\right] = p(1-p)\]</div>
</section>
</div><p>Bernoulli random variables, named after the Swiss mathematician <a class="reference external" href="https://en.wikipedia.org/wiki/Jacob_Bernoulli">Jacob Bernoulli</a>, have two states: either <code class="docutils literal notranslate"><span class="pre">1</span></code> or <code class="docutils literal notranslate"><span class="pre">0</span></code>. The probability of getting <code class="docutils literal notranslate"><span class="pre">1</span></code> is <span class="math notranslate nohighlight">\(p\)</span>, while the likelihood of getting a value of <code class="docutils literal notranslate"><span class="pre">0</span></code> is <span class="math notranslate nohighlight">\(1 − p\)</span>. Bernoulli random variables model many binary events: coin flips (H or T), binary bits (1 or 0), true or false, yes or no, present or absent, etc.</p>
<div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c"># load the distributions package, and some other stuff</span>
<span class="k">using</span> <span class="n">Distributions</span>
<span class="k">using</span> <span class="n">Statistics</span>
<span class="k">using</span> <span class="n">PrettyTables</span>

<span class="c"># Details of Bernoulli distribution: </span>
<span class="c"># https://juliastats.org/Distributions.jl/stable/univariate/#Discrete-Distributions</span>

<span class="c"># setup constants -</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.64</span><span class="p">;</span>
<span class="n">number_of_samples</span> <span class="o">=</span> <span class="mi">100</span><span class="p">;</span>

<span class="c"># build a Bernoulli distribution</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">Bernoulli</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

<span class="c"># sample (check expectation, and variance)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">number_of_samples</span><span class="p">);</span>

<span class="c"># build a table -</span>
<span class="n">data_for_table</span> <span class="o">=</span> <span class="kt">Array</span><span class="p">{</span><span class="kt">Any</span><span class="p">,</span><span class="mi">2</span><span class="p">}(</span><span class="nb">undef</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">table_header</span> <span class="o">=</span> <span class="p">[</span><span class="s">&quot;&quot;</span><span class="p">,</span> <span class="s">&quot;E(X)&quot;</span><span class="p">,</span> <span class="s">&quot;Var(X)&quot;</span><span class="p">]</span>

<span class="c"># row 1: model</span>
<span class="n">data_for_table</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="s">&quot;model&quot;</span>
<span class="n">data_for_table</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">d</span><span class="p">);</span>
<span class="n">data_for_table</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">var</span><span class="p">(</span><span class="n">d</span><span class="p">);</span>

<span class="c"># row 2: samples</span>
<span class="n">data_for_table</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="s">&quot;samples&quot;</span>
<span class="n">data_for_table</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">samples</span><span class="p">);</span>
<span class="n">data_for_table</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">var</span><span class="p">(</span><span class="n">samples</span><span class="p">);</span>
<span class="n">pretty_table</span><span class="p">(</span><span class="n">data_for_table</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="n">table_header</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="binomial-random-variable">
<h4>Binomial random variable<a class="headerlink" href="#binomial-random-variable" title="Permalink to this headline">#</a></h4>
<p>The binomial distribution is the probability of getting exactly <span class="math notranslate nohighlight">\(k\)</span> successes in <span class="math notranslate nohighlight">\(n\)</span> independent Bernoulli trials, e.g., the chance of getting four heads in 6 coin tosses (<a class="reference internal" href="#defn-pmf-binomial">Definition 9</a>):</p>
<div class="proof definition admonition" id="defn-pmf-binomial">
<p class="admonition-title"><span class="caption-number">Definition 9 </span> (Binomial Random Variable)</p>
<section class="definition-content" id="proof-content">
<p>Suppose we perform repeated Bernoulli trials <span class="math notranslate nohighlight">\(X(\Omega) \in [0,1]^n\)</span>, i.e., <span class="math notranslate nohighlight">\(n\)</span> trials of an independent binary experiment. The probability of getting exactly <span class="math notranslate nohighlight">\(k\)</span> successes in <span class="math notranslate nohighlight">\(n\)</span> independent Bernoulli trials is governed by the binomial probability mass function:</p>
<div class="math notranslate nohighlight">
\[p_{X}(k) = \binom{n}{k}p^{k}\left(1-p\right)^{n-k}\qquad{k=0,1,\dots,n}\]</div>
<p>where <span class="math notranslate nohighlight">\(k\)</span> denotes the number of successes in <span class="math notranslate nohighlight">\(n\)</span> independent experiments, the binomial parameter <span class="math notranslate nohighlight">\(0&lt;p&lt;1\)</span> is the probability
of a successful trial and:</p>
<div class="math notranslate nohighlight">
\[\binom{n}{k} = \frac{n!}{k!\left(n-k\right)!}\]</div>
<p>is the binomial coefficient. The expectation of a binomial random variable is given by:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\left[X\right] = np\]</div>
<p>while the variance <span class="math notranslate nohighlight">\(\text{Var}(X)\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\text{Var}\left[X\right] = np(1-p)\]</div>
</section>
</div><div class="highlight-julia notranslate"><div class="highlight"><pre><span></span><span class="c"># load the distributions package, and some other stuff</span>
<span class="k">using</span> <span class="n">Distributions</span>
<span class="k">using</span> <span class="n">Statistics</span>
<span class="k">using</span> <span class="n">PrettyTables</span>

<span class="c"># Details of Binomial distribution: </span>
<span class="c"># https://juliastats.org/Distributions.jl/stable/univariate/#Distributions.Binomial</span>

<span class="c"># setup constants -</span>
<span class="n">number_of_trials</span> <span class="o">=</span> <span class="mi">100</span><span class="p">;</span>
<span class="n">p</span> <span class="o">=</span> <span class="mf">0.64</span><span class="p">;</span>
<span class="n">number_of_samples</span> <span class="o">=</span> <span class="mi">100</span><span class="p">;</span>

<span class="c"># build a Bernoulli distribution</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">Binomial</span><span class="p">(</span><span class="n">number_of_trials</span><span class="p">,</span><span class="n">p</span><span class="p">)</span>

<span class="c"># sample (check expectation, and variance)</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">rand</span><span class="p">(</span><span class="n">d</span><span class="p">,</span><span class="n">number_of_samples</span><span class="p">);</span>

<span class="c"># build a table -</span>
<span class="n">data_for_table</span> <span class="o">=</span> <span class="kt">Array</span><span class="p">{</span><span class="kt">Any</span><span class="p">,</span><span class="mi">2</span><span class="p">}(</span><span class="nb">undef</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">table_header</span> <span class="o">=</span> <span class="p">[</span><span class="s">&quot;&quot;</span><span class="p">,</span> <span class="s">&quot;E(X)&quot;</span><span class="p">,</span> <span class="s">&quot;Var(X)&quot;</span><span class="p">]</span>

<span class="c"># row 1: model</span>
<span class="n">data_for_table</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="s">&quot;model&quot;</span>
<span class="n">data_for_table</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">d</span><span class="p">);</span>
<span class="n">data_for_table</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">var</span><span class="p">(</span><span class="n">d</span><span class="p">);</span>

<span class="c"># row 2: samples</span>
<span class="n">data_for_table</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="s">&quot;samples&quot;</span>
<span class="n">data_for_table</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">mean</span><span class="p">(</span><span class="n">samples</span><span class="p">);</span>
<span class="n">data_for_table</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">var</span><span class="p">(</span><span class="n">samples</span><span class="p">);</span>
<span class="n">pretty_table</span><span class="p">(</span><span class="n">data_for_table</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="n">table_header</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="geometric-random-variable">
<h4>Geometric random variable<a class="headerlink" href="#geometric-random-variable" title="Permalink to this headline">#</a></h4>
<p>Geometric random variables are a type of discrete probability distribution that models the number of trials required to obtain the first success in a sequence of independent Bernoulli trials (<a class="reference internal" href="#defn-pmf-geometric">Definition 10</a>):</p>
<div class="proof definition admonition" id="defn-pmf-geometric">
<p class="admonition-title"><span class="caption-number">Definition 10 </span> (Geometric Random Variable)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be a geometric random variable. The probability mass function for a geometric random variable is given by:</p>
<div class="math notranslate nohighlight">
\[p_{X}(k) = (1-p)^{(k-1)}p\qquad{k=1,2,\dots}\]</div>
<p>where <span class="math notranslate nohighlight">\(p\)</span> denotes the geometric parameter <span class="math notranslate nohighlight">\(0&lt;p&lt;1\)</span>. The expectation of a geometric random variable <span class="math notranslate nohighlight">\(X\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\left[X\right] = \frac{1}{p}\]</div>
<p>while the variance <span class="math notranslate nohighlight">\(\text{Var}(X)\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\text{Var}\left[X\right] = \frac{1-p}{p^2}\]</div>
</section>
</div></section>
<section id="poisson-random-variable">
<h4>Poisson random variable<a class="headerlink" href="#poisson-random-variable" title="Permalink to this headline">#</a></h4>
<p>Poisson random variables are a type of discrete probability distribution that models the number of occurrences of an event in a fixed interval of time or space (<a class="reference internal" href="#defn-pmf-poisson">Definition 11</a>):</p>
<div class="proof definition admonition" id="defn-pmf-poisson">
<p class="admonition-title"><span class="caption-number">Definition 11 </span> (Poisson Random Variable)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> be a Poisson random variable. The probability mass function for a Poisson random variable is given by:</p>
<div class="math notranslate nohighlight">
\[p_{X}(x) = \frac{\lambda^{x}}{x!}\exp\left(-\lambda\right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda&gt;0\)</span> denotes the Poisson parameter, and <span class="math notranslate nohighlight">\(!\)</span> denotes the factorial function. The expectation of a Poisson random variable <span class="math notranslate nohighlight">\(X\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\mathbb{E}\left[X\right] = \lambda\]</div>
<p>while the variance <span class="math notranslate nohighlight">\(\text{Var}(X)\)</span> is given by:</p>
<div class="math notranslate nohighlight">
\[\text{Var}\left[X\right] = \lambda\]</div>
</section>
</div><p>Poisson random variables estimate how likely something will happen <span class="math notranslate nohighlight">\(x\)</span> number of times in a fixed interval, e.g., the number of car crashes in a city of a given size or the number of cheeseburgers sold at a fast-food chain on a Friday night.</p>
</section>
</section>
</section>
<section id="moments-of-a-random-variable">
<h2>Moments of a random variable<a class="headerlink" href="#moments-of-a-random-variable" title="Permalink to this headline">#</a></h2>
<p>Moments of a random variable are a way to summarize its distribution and provide important information about its properties. Specifically, moments are mathematical quantities that describe the shape, center, and spread of a distribution, and they can be used to calculate other statistical measures such as variance and skewness.Let’s look at the first and second moments of a random variable, namely the <a class="reference internal" href="#content-references-random-variable-expectation"><span class="std std-ref">Expectation</span></a> and the <a class="reference internal" href="#content-references-random-variable-variance"><span class="std std-ref">Variance</span></a>.</p>
<section id="expectation">
<span id="content-references-random-variable-expectation"></span><h3>Expectation<a class="headerlink" href="#expectation" title="Permalink to this headline">#</a></h3>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Expected_value">expectation</a> of a discrete random variable <span class="math notranslate nohighlight">\(X\)</span> measures the central tendency of the values of that random variable (<a class="reference internal" href="#defn-discrete-random-variable-expectation">Definition 12</a>):</p>
<div class="proof definition admonition" id="defn-discrete-random-variable-expectation">
<p class="admonition-title"><span class="caption-number">Definition 12 </span> (Expectation discrete random variable)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> denote a discrete random variable with the probability space <span class="math notranslate nohighlight">\(\left(\Omega,\mathcal{F}, P\right)\)</span>, where <span class="math notranslate nohighlight">\(\Omega\)</span> denotes the sample space, <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> denotes the event space, and <span class="math notranslate nohighlight">\(P\)</span> denotes the probability measure. Then, the expected value of the random variable <span class="math notranslate nohighlight">\(X\)</span> is given by:</p>
<div class="math notranslate nohighlight" id="equation-eqn-expectation">
<span class="eqno">(12)<a class="headerlink" href="#equation-eqn-expectation" title="Permalink to this equation">#</a></span>\[\mathbb{E}\left[X\right] = \sum_{x\in\Omega}xp_{X}(x)\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> denotes a value for the discrete random variable <span class="math notranslate nohighlight">\(X\)</span>, and <span class="math notranslate nohighlight">\(p_{X}(x)\)</span> denotes the probability of <span class="math notranslate nohighlight">\(X=x\)</span>. The value of <span class="math notranslate nohighlight">\(p_{X}(x)\)</span> is governed by a <a class="reference external" href="https://en.wikipedia.org/wiki/Probability_mass_function">Probability Mass Function (PMF)</a>.</p>
</section>
</div><p>The expectation of a discrete random variable has a few interesting properties (<a class="reference internal" href="#obs-expectation-props">Observation 3</a>):</p>
<div class="proof observation admonition" id="obs-expectation-props">
<p class="admonition-title"><span class="caption-number">Observation 3 </span> (Properties of expectation)</p>
<section class="observation-content" id="proof-content">
<p>The expectation of a random variable <span class="math notranslate nohighlight">\(X\)</span> has several useful (and important) properties:</p>
<ol class="simple">
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}\left(c\right) = c\)</span> for any constant <span class="math notranslate nohighlight">\(c\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}\left(cX\right) = c\times\mathbb{E}\left(X\right)\)</span> for any constant <span class="math notranslate nohighlight">\(c\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}\left(g(X)\right) = \sum_{x\in{X(\Omega)}}g(x)p_{X}(x)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}\left(g(X)+h(X)\right) = \mathbb{E}(g(X)) + \mathbb{E}(h(X))\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}\left(X+c\right) = \mathbb{E}(X) + c\)</span> for any constant <span class="math notranslate nohighlight">\(c\)</span></p></li>
</ol>
</section>
</div></section>
<section id="variance">
<span id="content-references-random-variable-variance"></span><h3>Variance<a class="headerlink" href="#variance" title="Permalink to this headline">#</a></h3>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Variance">variance</a> measures the expected dispersion for
individual values of a random variable <span class="math notranslate nohighlight">\(X\)</span>, i.e., the average distance that values of <span class="math notranslate nohighlight">\(X\)</span> are spread out from their expected value (<a class="reference internal" href="#defn-discrete-random-variable-variance">Definition 13</a>):</p>
<div class="proof definition admonition" id="defn-discrete-random-variable-variance">
<p class="admonition-title"><span class="caption-number">Definition 13 </span> (Variance discrete random variable)</p>
<section class="definition-content" id="proof-content">
<p>Let <span class="math notranslate nohighlight">\(X\)</span> denote a discrete random variable with the probability space <span class="math notranslate nohighlight">\(\left(\Omega,\mathcal{F},P\right)\)</span>, where <span class="math notranslate nohighlight">\(\Omega\)</span> denotes the sample space, <span class="math notranslate nohighlight">\(\mathcal{F}\)</span> denotes the event space, and <span class="math notranslate nohighlight">\(P\)</span> denotes the probability measure. Then, the variance of the random variable <span class="math notranslate nohighlight">\(X\)</span> is given by:</p>
<div class="math notranslate nohighlight" id="equation-eqn-variance">
<span class="eqno">(13)<a class="headerlink" href="#equation-eqn-variance" title="Permalink to this equation">#</a></span>\[\text{Var}(X) = \mathbb{E}\Bigl[(X-\mu)^{2}\Bigr]\]</div>
<p>where <span class="math notranslate nohighlight">\(\mu = \mathbb{E}(X)\)</span> denotes the expected value of the random variable <span class="math notranslate nohighlight">\(X\)</span>.</p>
</section>
</div><p>The variance of a discrete random variable has a few interesting properties (<a class="reference internal" href="#obs-variances-var">Observation 4</a>):</p>
<div class="proof observation admonition" id="obs-variances-var">
<p class="admonition-title"><span class="caption-number">Observation 4 </span> (Properties of variance)</p>
<section class="observation-content" id="proof-content">
<p>The variance of a random variable <span class="math notranslate nohighlight">\(X\)</span> has a few interesting (and important) properties:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{Var}(X) = \mathbb{E}\left(X^{2}\right) - \mathbb{E}\left(X\right)^2\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Var}(cX) = {c^2}\text{Var}(X)\)</span> for any constant <span class="math notranslate nohighlight">\(c\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\text{Var}(X+c) = \text{Var}(X)\)</span> for any constant <span class="math notranslate nohighlight">\(c\)</span></p></li>
</ul>
</section>
</div><p>The more common quantity that is used to measure dispersion, the standard deviation <span class="math notranslate nohighlight">\(\sigma\)</span>, is related to the variance: <span class="math notranslate nohighlight">\(\sigma_{X} = \sqrt{\text{Var}(X)}\)</span>.</p>
</section>
</section>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="summary">
<h1>Summary<a class="headerlink" href="#summary" title="Permalink to this headline">#</a></h1>
<p>Toward understanding uncertain rational decisions, in this lecture we introduced the <a class="reference external" href="https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem">von Neumann-Morgenstern theorem</a> and then discussed tools to model uncertainty:</p>
<ul class="simple">
<li><p><a class="reference internal" href="#content-references-vnm-theorem"><span class="std std-ref">The von Neumann-Morgenstern theorem</span></a> is a fundamental concept in decision theory that provides a mathematical foundation for expected utility theory. It states that if an individual’s preferences over uncertain outcomes satisfy certain axioms, then there exists a utility function that represents those preferences. This theorem has important implications for understanding how people make decisions under uncertainty and designing mechanisms to elicit and aggregate individual preferences.</p></li>
<li><p><a class="reference internal" href="#content-references-random-variables-probability"><span class="std std-ref">Random variables and probability</span></a> are central concepts in understanding uncertainty.  Random variables represent quantities that can take on different values with a particular probability distribution. They are used to model uncertainty and randomness in various fields, including finance, physics, and engineering. Probability theory provides a framework for understanding and quantifying the likelihood of different events occurring based on the underlying distribution of the random variables involved.</p></li>
</ul>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./unit-2-uncertainty"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="uncertainty-landing.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Decision-making in uncertain situations</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="risk.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Risk and Risk Aversion</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Varner<br/>
  
      &copy; Copyright 2023.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>